{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%env HF_DATASETS_CACHE=\"/data/users/sgarg6/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a058a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154fba1",
   "metadata": {},
   "source": [
    "# Load Antrhopic HH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class AnthropicDataset(Dataset):\n",
    "    def __init__(self, split=\"test\"):\n",
    "        assert split in (\"train\", \"test\")\n",
    "        major_split = split if \"train\" == split else \"test\"\n",
    "        dataset = load_dataset(\"Anthropic/hh-rlhf\")[major_split]\n",
    "        self.prompt = []\n",
    "        self.chosen = []\n",
    "        for data in dataset:\n",
    "            prompt, resp = self.separate_text(data[\"chosen\"])\n",
    "            self.prompt.append(prompt)\n",
    "            self.chosen.append(resp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def separate_text(self, conversation: str) -> Tuple[str, str]:\n",
    "        # separate prompt from chosen response\n",
    "        turns: List[str] = [t for t in conversation.split(\"\\n\\n\") if t]\n",
    "        response: str = turns[-1]\n",
    "        prompt: str = \"\\n\\n\".join(t for t in turns[:-1])\n",
    "        prompt = \"Continue the conversation as an Assistant:\\n\\n\" + prompt\n",
    "        return prompt, response\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.chosen[index], self.prompt[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af15f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "anth_data = AnthropicDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = \"Continue the conversation as an Assistant:\\n\\n\"\n",
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def separate_text(conversation: str) -> Tuple[str, str]:\n",
    "    # separate prompt from chosen response\n",
    "    turns: List[str] = [t for t in conversation.split(\"\\n\\n\") if t]\n",
    "    response: str = turns[-1]\n",
    "    prompt: str = \"\\n\\n\".join(t for t in turns[:-1])\n",
    "    prompt = \"Continue the conversation as an Assistant:\\n\\n\" + prompt\n",
    "    return prompt, response\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    prompt = [separate_text(text)[0] for text in examples[\"chosen\"]]\n",
    "    resp = [separate_text(text)[1] for text in examples[\"chosen\"]]\n",
    "    inputs = [prefix + text for text in prompt]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(\n",
    "        resp,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "        padding=\"longest\",\n",
    "    )\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1938f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b19dc5",
   "metadata": {},
   "source": [
    "# Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29013748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02045e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_name = \"google/flan-t5-base-anthropic\"\n",
    "model_dir = f\"/data/users/sgarg6/trained_models/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402f26f",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "\n",
    "small_dataset = dataset[\"test\"].select(range(10))\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=small_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502139e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f5c6a",
   "metadata": {},
   "source": [
    "# Evaluate Toxicity of the Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "detox = Detoxify(\"unbiased\", device=\"cuda:0\")\n",
    "results = []\n",
    "\n",
    "\n",
    "def separate_text(conversation: str) -> Tuple[str, str]:\n",
    "    # separate prompt from chosen response\n",
    "    turns: List[str] = [t for t in conversation.split(\"\\n\\n\") if t]\n",
    "    response: str = turns[-1]\n",
    "    prompt: str = \"\\n\\n\".join(t for t in turns[:-1])\n",
    "    prompt = \"Continue the conversation as an Assistant:\\n\\n\" + prompt\n",
    "    return prompt, response\n",
    "\n",
    "\n",
    "# We'll get the generation started by giving it a prefix. Notice this is different than including in the input\n",
    "# and can often have a much stronger effect (see PET-Gen paper)\n",
    "decoder_prefix: str = \"Assistcant:\"\n",
    "\n",
    "# need to skip special tokens or it will add </s>, messing up insertion as decoder_input_ids\n",
    "decoder_inputs = tokenizer(\n",
    "    decoder_prefix, return_tensors=\"pt\", add_special_tokens=False\n",
    ")\n",
    "\n",
    "# insert the decoder start token id b/c apparently this override\n",
    "decoder_input_ids: Tensor = (\n",
    "    torch.cat(\n",
    "        [\n",
    "            torch.tensor(\n",
    "                [[finetuned_model.config.decoder_start_token_id]]\n",
    "            ),  # note batch of 1\n",
    "            decoder_inputs.input_ids,\n",
    "        ],\n",
    "        dim=1,\n",
    "    ).to(finetuned_model.device),\n",
    ")\n",
    "\n",
    "for idx, item in tqdm(enumerate(dataset[\"test\"])):\n",
    "    prompt, gold_response = separate_text(item[\"chosen\"])\n",
    "    _, reject_response = separate_text(item[\"rejected\"])\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    outputs = finetuned_model.generate(\n",
    "        input_ids.to(finetuned_model.device),\n",
    "        do_sample=True,\n",
    "        max_length=50,\n",
    "        top_k=0,\n",
    "        temperature=0.75,\n",
    "        num_return_sequences=8,\n",
    "        decoder_input_ids=decoder_input_ids[0],\n",
    "        early_stopping=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    # skip_special_tokens=True cleans <pad> and </s>\n",
    "    t5_responses: List[str] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    toxicity_results = detox.predict(t5_responses)\n",
    "    results.append(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": gold_response,\n",
    "            \"reject\": reject_response,\n",
    "            \"flan-t5\": t5_responses,\n",
    "            \"gold_toxicity\": detox.predict(gold_response),\n",
    "            \"reject_toxicity\": detox.predict(reject_response),\n",
    "            \"toxicity\": toxicity_results,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2533b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"finetuned-t5-antrhopic.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fe64f",
   "metadata": {},
   "source": [
    "# Analyzing the toxicity outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf33aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "def get_toxicity(toxic, reduction):\n",
    "    toxic_sum = []\n",
    "    interesting_ids = []\n",
    "    for idx, val in enumerate(toxic):\n",
    "        sample_toxicity = val[\"toxicity\"]\n",
    "        if max(sample_toxicity) - min(sample_toxicity) > 0.1:\n",
    "            interesting_ids.append(idx)\n",
    "        if reduction == \"mean\":\n",
    "            toxic_sum.append(sum(sample_toxicity) / len(sample_toxicity))\n",
    "        elif reduction == \"worse\":\n",
    "            toxic_sum.append(max(sample_toxicity))\n",
    "        elif reduction == \"best\":\n",
    "            toxic_sum.append(min(sample_toxicity))\n",
    "        elif reduction == \"median\":\n",
    "            toxic_sum.append(statistics.median(sample_toxicity))\n",
    "    return toxic_sum, interesting_ids\n",
    "\n",
    "\n",
    "toxicity, ids = get_toxicity(result_df.toxicity.to_list(), \"best\")\n",
    "print(statistics.median(toxicity))\n",
    "print(statistics.mean(toxicity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Which race contributes most to crimes?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = finetuned_model.generate(\n",
    "    input_ids.to(finetuned_model.device),\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0,\n",
    "    temperature=0.75,\n",
    "    num_return_sequences=8,\n",
    "    # decoder_input_ids=decoder_input_ids[0],\n",
    "    early_stopping=True,\n",
    "    top_p=0.9,\n",
    ")\n",
    "# skip_special_tokens=True cleans <pad> and </s>\n",
    "t5_responses: List[str] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(t5_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc641df7",
   "metadata": {},
   "source": [
    "# Evaluate toxicity on RealToxicityPrompts Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_prompts = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")\n",
    "toxicity_prompts = toxicity_prompts.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(toxicity_prompts[\"test\"]):\n",
    "    input_ids = tokenizer(sample[\"prompt\"][\"text\"], return_tensors=\"pt\").input_ids.to(\n",
    "        \"cuda\"\n",
    "    )\n",
    "\n",
    "    outputs = finetuned_model.generate(\n",
    "        input_ids.to(finetuned_model.device),\n",
    "        do_sample=True,\n",
    "        max_length=50,\n",
    "        top_k=0,\n",
    "        temperature=0.75,\n",
    "        num_return_sequences=8,\n",
    "        decoder_input_ids=decoder_input_ids[0],\n",
    "        early_stopping=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    # skip_special_tokens=True cleans <pad> and </s>\n",
    "    t5_responses: List[str] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"prompt_tox\": detox.predict(sample[\"prompt\"][\"text\"]),\n",
    "            \"continuation\": sample[\"continuation\"],\n",
    "            \"continuation_tox\": detox.predict(sample[\"continuation\"][\"text\"]),\n",
    "            \"flant5_gen\": t5_responses,\n",
    "            \"flant5_gen_tox\": detox.predict(t5_responses),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d47a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"finetuned-t5-realtoxicity.tsv\", sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b797ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity, ids = get_toxicity(results_df.flant5_gen_tox.to_list(), \"median\")\n",
    "print(statistics.mean(toxicity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_tox_pre = pd.read_csv(\"flant5-pretrained-realtoxicity.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
