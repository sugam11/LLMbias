{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f099387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454f4490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_DATASETS_CACHE=\"/data/users/sgarg6/hf_cache\"\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%env HF_DATASETS_CACHE=\\\"/data/users/sgarg6/hf_cache\\\"\";\n",
       "                var nbb_formatted_code = \"%env HF_DATASETS_CACHE=\\\"/data/users/sgarg6/hf_cache\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env HF_DATASETS_CACHE=\"/data/users/sgarg6/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e37d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"model_name = \\\"t5-base\\\"\";\n",
       "                var nbb_formatted_code = \"model_name = \\\"t5-base\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03240cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import torch\\nfrom transformers import AutoTokenizer, GPT2ForSequenceClassification\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"microsoft/DialogRPT-updown\\\")\\nmodel = GPT2ForSequenceClassification.from_pretrained(\\\"microsoft/DialogRPT-updown\\\")\\n\\ninputs = tokenizer(\\\"Hello, my dog is cute\\\", return_tensors=\\\"pt\\\")\\n\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\n\\npredicted_class_id = logits.argmax().item()\\n\\n# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\\n# model = GPT2ForSequenceClassification.from_pretrained(\\\"microsoft/DialogRPT-updown\\\", num_labels=1)\\n\\n# labels = torch.tensor([1])\\n# loss = model(**inputs, labels=labels).loss\";\n",
       "                var nbb_formatted_code = \"import torch\\nfrom transformers import AutoTokenizer, GPT2ForSequenceClassification\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"microsoft/DialogRPT-updown\\\")\\nmodel = GPT2ForSequenceClassification.from_pretrained(\\\"microsoft/DialogRPT-updown\\\")\\n\\ninputs = tokenizer(\\\"Hello, my dog is cute\\\", return_tensors=\\\"pt\\\")\\n\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\n\\npredicted_class_id = logits.argmax().item()\\n\\n# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\\n# model = GPT2ForSequenceClassification.from_pretrained(\\\"microsoft/DialogRPT-updown\\\", num_labels=1)\\n\\n# labels = torch.tensor([1])\\n# loss = model(**inputs, labels=labels).loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "# model = GPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\", num_labels=1)\n",
    "\n",
    "# labels = torch.tensor([1])\n",
    "# loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bd2fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2981]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"logits\";\n",
       "                var nbb_formatted_code = \"logits\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa3e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\n# inputs = tokenizer(\\n#     \\\"scalar evaluation score: \\\\n\\\\n Human: What kind of noises did dinosaurs make?\\\\n\\\\nAssistant: Humans and dinosaurs didn\\u2019t live at the same time, so it\\u2019s really hard to say. The best place to find out what noises dinosaurs made would be\\\",\\n#     return_tensors=\\\"pt\\\",\\n# )\\n# outputs = model.generate(**inputs)\\n# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\";\n",
       "                var nbb_formatted_code = \"# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\n# inputs = tokenizer(\\n#     \\\"scalar evaluation score: \\\\n\\\\n Human: What kind of noises did dinosaurs make?\\\\n\\\\nAssistant: Humans and dinosaurs didn\\u2019t live at the same time, so it\\u2019s really hard to say. The best place to find out what noises dinosaurs made would be\\\",\\n#     return_tensors=\\\"pt\\\",\\n# )\\n# outputs = model.generate(**inputs)\\n# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(\n",
    "#     \"scalar evaluation score: \\n\\n Human: What kind of noises did dinosaurs make?\\n\\nAssistant: Humans and dinosaurs didn’t live at the same time, so it’s really hard to say. The best place to find out what noises dinosaurs made would be\",\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# outputs = model.generate(**inputs)\n",
    "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4b5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414\n",
      "Found cached dataset json (/soe/sgarg6/course_work/244_nlp/LLMbias/\"/data/users/sgarg6/hf_cache\"/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59b440edc9b47f1998e5abf4e5585cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from datasets import load_dataset\\n\\ndataset = load_dataset(\\\"Anthropic/hh-rlhf\\\")\";\n",
       "                var nbb_formatted_code = \"from datasets import load_dataset\\n\\ndataset = load_dataset(\\\"Anthropic/hh-rlhf\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d90b7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 160800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 8552\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"dataset\";\n",
       "                var nbb_formatted_code = \"dataset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025d1672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from torch.utils.data import Dataset\\n\\n\\nclass AnthropicDataset(Dataset):\\n    def __init__(self, split=\\\"test\\\"):\\n        assert split in (\\\"train\\\", \\\"test\\\")\\n        major_split = split if \\\"train\\\" == split else \\\"test\\\"\\n        dataset = load_dataset(\\\"Anthropic/hh-rlhf\\\")[major_split]\\n        self.data = []\\n        for data in dataset:\\n            self.data.append((data[\\\"chosen\\\"], 1))\\n            self.data.append((data[\\\"rejected\\\"], 0))\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, index):\\n        sample, label = self.data[index]\\n\\n        return sample, label\";\n",
       "                var nbb_formatted_code = \"from torch.utils.data import Dataset\\n\\n\\nclass AnthropicDataset(Dataset):\\n    def __init__(self, split=\\\"test\\\"):\\n        assert split in (\\\"train\\\", \\\"test\\\")\\n        major_split = split if \\\"train\\\" == split else \\\"test\\\"\\n        dataset = load_dataset(\\\"Anthropic/hh-rlhf\\\")[major_split]\\n        self.data = []\\n        for data in dataset:\\n            self.data.append((data[\\\"chosen\\\"], 1))\\n            self.data.append((data[\\\"rejected\\\"], 0))\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, index):\\n        sample, label = self.data[index]\\n\\n        return sample, label\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class AnthropicDataset(Dataset):\n",
    "    def __init__(self, split=\"test\"):\n",
    "        assert split in (\"train\", \"test\")\n",
    "        major_split = split if \"train\" == split else \"test\"\n",
    "        dataset = load_dataset(\"Anthropic/hh-rlhf\")[major_split]\n",
    "        self.data = []\n",
    "        for data in dataset:\n",
    "            self.data.append((data[\"chosen\"], 1))\n",
    "            self.data.append((data[\"rejected\"], 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, label = self.data[index]\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca1f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414\n",
      "Found cached dataset json (/soe/sgarg6/course_work/244_nlp/LLMbias/\"/data/users/sgarg6/hf_cache\"/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc08542419441dc851397cbcd6c891c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414\n",
      "Found cached dataset json (/soe/sgarg6/course_work/244_nlp/LLMbias/\"/data/users/sgarg6/hf_cache\"/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4701b08f77db491687f96ea47b230863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"train_data = AnthropicDataset(\\\"train\\\")\\ntest_data = AnthropicDataset(\\\"test\\\")\";\n",
       "                var nbb_formatted_code = \"train_data = AnthropicDataset(\\\"train\\\")\\ntest_data = AnthropicDataset(\\\"test\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = AnthropicDataset(\"train\")\n",
    "test_data = AnthropicDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fc55d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\";\n",
       "                var nbb_formatted_code = \"device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ae6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"def collate_data(data):\\n    text_samples = [sample[0] for sample in data]\\n    labels = torch.tensor([sample[1] for sample in data], dtype=torch.float).to(device)\\n    tokens = tokenizer(\\n        text_samples,\\n        return_tensors=\\\"pt\\\",\\n        truncation=True,\\n        padding=True,\\n        max_length=512,\\n    ).to(device)\\n    return tokens, labels\";\n",
       "                var nbb_formatted_code = \"def collate_data(data):\\n    text_samples = [sample[0] for sample in data]\\n    labels = torch.tensor([sample[1] for sample in data], dtype=torch.float).to(device)\\n    tokens = tokenizer(\\n        text_samples,\\n        return_tensors=\\\"pt\\\",\\n        truncation=True,\\n        padding=True,\\n        max_length=512,\\n    ).to(device)\\n    return tokens, labels\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collate_data(data):\n",
    "    text_samples = [sample[0] for sample in data]\n",
    "    labels = torch.tensor([sample[1] for sample in data], dtype=torch.float).to(device)\n",
    "    tokens = tokenizer(\n",
    "        text_samples,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd66039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"from torch.utils.data import DataLoader\\n\\n# train_dataloader = DataLoader(train_data, collate_fn=collate_data, batch_size=2)\";\n",
       "                var nbb_formatted_code = \"from torch.utils.data import DataLoader\\n\\n# train_dataloader = DataLoader(train_data, collate_fn=collate_data, batch_size=2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(train_data, collate_fn=collate_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9df95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"def free_memory():\\n    import gc\\n\\n    torch.cuda.empty_cache()\\n    gc.collect()\";\n",
       "                var nbb_formatted_code = \"def free_memory():\\n    import gc\\n\\n    torch.cuda.empty_cache()\\n    gc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def free_memory():\n",
    "    import gc\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "458ea6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"from tqdm import tqdm\\nimport time\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import f1_score\\n\\n\\ndef evaluate(model, data_loader, criterion):\\n    model.eval()\\n    total_loss = 0.0\\n    start_time = time.time()\\n    out = []\\n    label = []\\n    for X, y in tqdm(data_loader):\\n        model.zero_grad()\\n        output = model(**X).logits\\n        loss = criterion(output.reshape(-1), y)\\n        total_loss += loss.item()\\n        pred = torch.sigmoid(output) > 0.5\\n        out.extend(pred.long().detach().tolist())\\n        label.extend(y.long().detach().tolist())\\n        del X, y, output, loss\\n        free_memory()\\n    acc = accuracy_score(label, out)\\n    f1 = f1_score(label, out)\\n    return total_loss, acc, f1\";\n",
       "                var nbb_formatted_code = \"from tqdm import tqdm\\nimport time\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import f1_score\\n\\n\\ndef evaluate(model, data_loader, criterion):\\n    model.eval()\\n    total_loss = 0.0\\n    start_time = time.time()\\n    out = []\\n    label = []\\n    for X, y in tqdm(data_loader):\\n        model.zero_grad()\\n        output = model(**X).logits\\n        loss = criterion(output.reshape(-1), y)\\n        total_loss += loss.item()\\n        pred = torch.sigmoid(output) > 0.5\\n        out.extend(pred.long().detach().tolist())\\n        label.extend(y.long().detach().tolist())\\n        del X, y, output, loss\\n        free_memory()\\n    acc = accuracy_score(label, out)\\n    f1 = f1_score(label, out)\\n    return total_loss, acc, f1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    out = []\n",
    "    label = []\n",
    "    for X, y in tqdm(data_loader):\n",
    "        model.zero_grad()\n",
    "        output = model(**X).logits\n",
    "        loss = criterion(output.reshape(-1), y)\n",
    "        total_loss += loss.item()\n",
    "        pred = torch.sigmoid(output) > 0.5\n",
    "        out.extend(pred.long().detach().tolist())\n",
    "        label.extend(y.long().detach().tolist())\n",
    "        del X, y, output, loss\n",
    "        free_memory()\n",
    "    acc = accuracy_score(label, out)\n",
    "    f1 = f1_score(label, out)\n",
    "    return total_loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9827e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"from tqdm import tqdm\\nimport time\\n\\n\\ndef train_step(data_loader, model, epoch, criterion, optimizer):\\n    model.train()\\n    total_loss = 0.0\\n    start_time = time.time()\\n    for batch, (X, y) in tqdm(enumerate(data_loader)):\\n        model.zero_grad()\\n        output = model(**X).logits\\n        loss = criterion(output.reshape(-1), y)\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\\n        optimizer.step()\\n        total_loss += loss.item()\\n        if batch % 50 == 0 and batch > 0:\\n            cur_loss = loss.item()\\n            elapsed = time.time() - start_time\\n            print(\\\"| epoch {:3d} |\\\" \\\" loss {:5.2f}\\\".format(epoch, cur_loss))\\n        del loss, X, y, output\\n        free_memory()\";\n",
       "                var nbb_formatted_code = \"from tqdm import tqdm\\nimport time\\n\\n\\ndef train_step(data_loader, model, epoch, criterion, optimizer):\\n    model.train()\\n    total_loss = 0.0\\n    start_time = time.time()\\n    for batch, (X, y) in tqdm(enumerate(data_loader)):\\n        model.zero_grad()\\n        output = model(**X).logits\\n        loss = criterion(output.reshape(-1), y)\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\\n        optimizer.step()\\n        total_loss += loss.item()\\n        if batch % 50 == 0 and batch > 0:\\n            cur_loss = loss.item()\\n            elapsed = time.time() - start_time\\n            print(\\\"| epoch {:3d} |\\\" \\\" loss {:5.2f}\\\".format(epoch, cur_loss))\\n        del loss, X, y, output\\n        free_memory()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def train_step(data_loader, model, epoch, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for batch, (X, y) in tqdm(enumerate(data_loader)):\n",
    "        model.zero_grad()\n",
    "        output = model(**X).logits\n",
    "        loss = criterion(output.reshape(-1), y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % 50 == 0 and batch > 0:\n",
    "            cur_loss = loss.item()\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"| epoch {:3d} |\" \" loss {:5.2f}\".format(epoch, cur_loss))\n",
    "        del loss, X, y, output\n",
    "        free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19d9de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Wed Mar  1 14:37:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    43W / 300W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"!nvidia-smi\";\n",
       "                var nbb_formatted_code = \"!nvidia-smi\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | loss  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [02:13,  1.39s/it]"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "\n",
    "best_val_loss = 99999\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_data\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_data\n",
    "    )\n",
    "    train_step(train_dataloader, model, epoch, criterion, optimizer)\n",
    "    val_loss, acc, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "    print(\"-\" * 89)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \"\n",
    "        \" acc {:5.2f}\".format(epoch, (time.time() - epoch_start_time), val_loss, acc)\n",
    "    )\n",
    "    print(f\"F1-score is {f1}\")\n",
    "    print(\"-\" * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        with open(\"./model.pt\", \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d219a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# class AnthropicRLHF(Dataset):\n",
    "#     \"\"\"\n",
    "#     The data are described in the paper:\n",
    "#         Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.\n",
    "#         If you find the data useful, please cite the paper.\n",
    "#         The data format is very simple -- each line of the jsonl files contains a pair of texts,\n",
    "#         one \"chosen\" and one \"rejected\".\n",
    "#     valid train size : 160780\n",
    "#     \"\"\"\n",
    "\n",
    "#     def preprocess_dialogue(self, text):\n",
    "#         \"\"\"\n",
    "#         trim prefix text to last two pairs\n",
    "#         Outlier example Assistant answered empty string:\n",
    "#             Assistant: Human: That makes sense, I agree with that, though there are many situations that\n",
    "#             aren't considered justice, like sending a kid to prison for life.  Human: You are completely\n",
    "#             missing the point of this conversation, and not understanding anything I am saying.  Human:\n",
    "#             And I don’t know if you’re trying to be funny, but it isn’t.\n",
    "#         \"\"\"\n",
    "#         last_two_convo = text.split(\"Human:\")[-2:]\n",
    "#         if len(last_two_convo[0]) == 0:\n",
    "#             return \"Human:\".join(last_two_convo)\n",
    "#         return \"Human: \" + \"Human:\".join(last_two_convo)\n",
    "\n",
    "#     def __init__(self, split=\"train\", sep_token=\"<sep>\") -> None:\n",
    "#         super().__init__()\n",
    "#         assert split in (\"train\", \"test\")\n",
    "#         if sep_token is None:\n",
    "#             sep_token = \" . \"\n",
    "#         self.pairs = []\n",
    "#         # using prompt as our index will allows us\n",
    "#         # to add additional generated prompt later\n",
    "#         major_split = split if \"train\" == split else \"test\"\n",
    "#         dataset = load_dataset(\"Anthropic/hh-rlhf\")[major_split]\n",
    "#         for data in dataset:\n",
    "#             processed = self.preprocess_dialogue(data[\"chosen\"])\n",
    "#             # roughly 20 of these are invalid conversation\n",
    "#             if \"Assistant\" not in processed:\n",
    "#                 continue\n",
    "#             prompt, pos_postfix = processed.split(\"Assistant:\", maxsplit=1)\n",
    "#             prompt = prompt.replace(\"Human: \", \"\").strip()\n",
    "#             pos_postfix = (\n",
    "#                 pos_postfix.replace(\"Human: \", sep_token)\n",
    "#                 .replace(\"\\n\\nAssistant: \", sep_token)\n",
    "#                 .strip()\n",
    "#             )\n",
    "#             processed = self.preprocess_dialogue(data[\"rejected\"])\n",
    "#             if \"Assistant\" not in processed:\n",
    "#                 continue\n",
    "#             _, neg_postfix = processed.split(\"Assistant:\", maxsplit=1)\n",
    "#             neg_postfix = (\n",
    "#                 neg_postfix.replace(\"Human: \", sep_token)\n",
    "#                 .replace(\"\\n\\nAssistant: \", sep_token)\n",
    "#                 .strip()\n",
    "#             )\n",
    "#             self.pairs.append((prompt, (pos_postfix.strip(), neg_postfix.strip())))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         context, pair = self.pairs[index]\n",
    "\n",
    "#         return context, [pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b069ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
